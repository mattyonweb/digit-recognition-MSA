@article{cortes1995support,
  added-at = {2015-09-17T16:32:09.000+0200},
  author = {Cortes, C. and Vapnik, V.},
  biburl = {https://www.bibsonomy.org/bibtex/22b1eb8bea07ae0156a53a4e9c6eac1df/nosebrain},
  interhash = {c223c465141618ad63aac5a6132280f7},
  intrahash = {2b1eb8bea07ae0156a53a4e9c6eac1df},
  journal = {Machine Learning},
  keywords = {classification margin soft support svm vector},
  pages = {273-297},
  timestamp = {2015-09-17T17:15:55.000+0200},
  title = {Support Vector Networks},
  volume = 20,
  year = 1995
}

@ARTICLE{uspsdataset,
  author={J. J. {Hull}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A database for handwritten text recognition research}, 
  year={1994},
  volume={16},
  number={5},
  pages={550-554},
  doi={10.1109/34.291440}
}

@inproceedings{pegasos,
author = {Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan},
title = {Pegasos: Primal Estimated Sub-GrAdient SOlver for SVM},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273598},
doi = {10.1145/1273496.1273598},
abstract = {We describe and analyze a simple and effective iterative algorithm for solving the optimization problem cast by Support Vector Machines (SVM). Our method alternates between stochastic gradient descent steps and projection steps. We prove that the number of iterations required to obtain a solution of accuracy ε is \~{O}(1/ε). In contrast, previous analyses of stochastic gradient descent methods require Ω (1/ε2) iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/λ, where λ is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is \~{O} (d/(λε)), where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach can seamlessly be adapted to employ non-linear kernels while working solely on the primal objective function. We demonstrate the efficiency and applicability of our approach by conducting experiments on large text classification problems, comparing our solver to existing state-of-the-art SVM solvers. For example, it takes less than 5 seconds for our solver to converge when solving a text classification problem from Reuters Corpus Volume 1 (RCV1) with 800,000 training examples.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

